{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3aXk1o_qRLu"
   },
   "source": [
    "# PyTorch - the Basics!\n",
    "\n",
    "Advanced Learning 2024\n",
    "\n",
    "Lee Carlin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O88JcDDEeVKl"
   },
   "source": [
    "## For SUBMISSION:\n",
    "~~~\n",
    "STUDENT ID: 208088815\n",
    "~~~\n",
    "~~~\n",
    "STUDENT GIT LINK: https://github.com/OrelRevivo1410/Adv.-computational-learning-and-data-analysis---52025\n",
    "~~~\n",
    "In Addition, don't forget to add your ID to the files:\n",
    "`PS0_PyTorch_basics_2024_ID_[208088815].ipynb`  \n",
    "`PS0_PyTorch_basics_2024_ID_[208088815].html`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCByaAJ1q-Gc"
   },
   "source": [
    "## 1. What is PyTorch?\n",
    "\n",
    "\n",
    "PyTorch is a popular open-source library for machine learning, particularly well-suited for deep learning applications.    \n",
    "Here's a breakdown of its key features:\n",
    "\n",
    "    \n",
    "\n",
    "*   **Deep Learning Framework:**  PyTorch provides tools and functionalities to  build and train complex neural networks.\n",
    "*   **Pythonic Interface:**  Known for its Python-like syntax, PyTorch is considered user-friendly and easy to learn.\n",
    "* **Flexibility:**  PyTorch offers both dynamic computational graphs (eager execution) and static graphs (graph mode) for model development.\n",
    "* **Production Ready:**  PyTorch provides features like TorchScript to transition models from development to production seamlessly.\n",
    "* **Scalability:**  PyTorch supports distributed training, enabling you to leverage multiple CPUs or GPUs to train models faster.\n",
    "* **Rich Ecosystem:**  A growing ecosystem of libraries and tools built on PyTorch expands its capabilities for tasks like computer vision, natural language processing, and model interpretability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0NFG56arhsD"
   },
   "source": [
    "We start by importing PyTorch's main objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "phCzuWkUqPRk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jL-emQP0xtkr"
   },
   "source": [
    "### PyTorch Main Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKs60PcGucH7"
   },
   "source": [
    "`torch.nn`:\n",
    "\n",
    "In PyTorch, the `torch.nn` module provides the essential building blocks you need to construct and train neural networks. It offers a comprehensive collection of classes and functions that streamline the deep learning development process.  \n",
    "  \n",
    "*Key Components:*  \n",
    "\n",
    "**Modules**:   \n",
    "These are the fundamental units that perform specific operations on data. They can be combined to create complex neural network architectures. Common examples include:   \n",
    "\n",
    "* Linear Layers (nn.Linear): Apply linear transformations (y = xA^T + b) for feeding data forward through the network.\n",
    "* Convolutional Layers (nn.Conv2d): Perform convolutions, especially useful for processing image data.\n",
    "* Activation Layers (nn.ReLU, nn.Sigmoid): Introduce non-linearity into the network, allowing it to learn complex patterns.\n",
    "* Normalization Layers (nn.BatchNorm2d): Normalize inputs to layers for faster and more stable training.\n",
    "* Recurrent Layers (nn.LSTM, nn.GRU): Handle sequential data like text or time series.\n",
    "* Dropout Layers (nn.Dropout): Introduce randomness by randomly dropping out neurons during training to prevent overfitting.\n",
    "* Many More: PyTorch offers a vast selection of modules catering to diverse neural network architectures.\n",
    "\n",
    "**Containers**:   \n",
    "These classes help you organize and structure your modules into hierarchical networks. They include:\n",
    "* nn.Sequential: Stacks modules in a linear sequence, making it easy to define simple neural networks.\n",
    "* nn.ModuleList: Holds other modules in a list, allowing for more flexible network structures.\n",
    "* nn.ModuleDict: Manages sub-modules with dictionary-like access for complex topologies.\n",
    "\n",
    "**Loss Functions:**   \n",
    "Functions that measure the error between the network's predictions and the ground truth labels. These guide the training process by calculating the gradients used to update the network's weights. Common examples include:\n",
    "* nn.CrossEntropyLoss: For multi-class classification problems.\n",
    "* nn.MSELoss: For mean squared error calculations in regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tw5uR0na0s8J"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzHi6U5zvteu"
   },
   "source": [
    "`torch.utils.data.DataLoader`:\n",
    "\n",
    "In PyTorch, `torch.utils.data.DataLoader` is a powerful tool that simplifies how you load and manage data for training your deep learning models. It acts as an iterator, efficiently providing batches of data during the training process.\n",
    "\n",
    "Here's a breakdown of its key functionalities:\n",
    "\n",
    "Data Management Abstraction:\n",
    "\n",
    "    Decouples data loading logic from your model training code, promoting cleaner and more maintainable code.\n",
    "    Handles complexities like batching, shuffling, and multi-processing data loading, freeing you from writing repetitive code.\n",
    "\n",
    "Efficient Batching:\n",
    "\n",
    "    Groups data samples (images, text, etc.) into batches of a specified size (batch_size). Batching improves computational efficiency by utilizing vectorized operations on GPUs.\n",
    "    Provides an optional collate_fn argument that allows you to customize how samples within a batch are combined. This can be useful for tasks like padding sequences to have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1715159495582,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "sbHK6GkR0vIH",
    "outputId": "57cc3fdd-175e-4631-a2e5-f1d679d79d03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sut-qO6F0x1X"
   },
   "source": [
    "In our course we use PyTorch version >=2.2.1.   \n",
    "`cu121` refers to CUDA 12.1, a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements for the execution of compute kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE9DKe-9s2aH"
   },
   "source": [
    "## 2. Tensors\n",
    "\n",
    "In PyTorch, tensors are the fundamental data structure. They are similar to NumPy arrays but with some key advantages for deep learning:\n",
    "\n",
    "* **Multi-dimensional Arrays**: Like NumPy arrays, tensors can have multiple dimensions, making them suitable for representing data like images (2D), videos (3D), or sequences of text (1D).\n",
    "\n",
    "* **Hardware Acceleration**: PyTorch tensors can be moved to and run on GPUs or other hardware accelerators, significantly speeding up computations compared to CPUs for deep learning tasks.\n",
    "\n",
    "* **Automatic Differentiation**:  A core feature in deep learning, automatic differentiation allows PyTorch to calculate gradients efficiently, which is essential for training neural networks.  Regular NumPy arrays don't inherently support this.\n",
    "\n",
    "* **Rich Functionality**: PyTorch offers a variety of operations specifically designed for tensors, making it convenient to manipulate and analyze data for deep learning models.\n",
    "\n",
    "In essence, tensors in PyTorch act as the workhorses for your deep learning models. They store and process the data that gets fed into your network, undergoes computations, and ultimately leads to predictions or outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrthmFbIfsXR"
   },
   "source": [
    "**PyTorch tensor operations:**\n",
    "\n",
    "PyTorch tensor operations are the fundamental building blocks for working with data in your deep learning models. These operations allow you to manipulate, analyze, and transform tensors in various ways. Here's a breakdown of some common categories:\n",
    "\n",
    "1. Arithmetic Operations:\n",
    "\n",
    "These operations perform element-wise calculations between tensors or a tensor and a scalar value. They include:\n",
    "\n",
    "    Addition (+)\n",
    "    Subtraction (-)\n",
    "    Multiplication (*)\n",
    "    Division (/)\n",
    "    Exponentiation (**)\n",
    "\n",
    "These operations can be used for simple calculations or combined to create more complex expressions.\n",
    "\n",
    "2. Comparison Operations:\n",
    "\n",
    "These operations compare elements between tensors or a tensor and a scalar value, resulting in a tensor of booleans (True or False) indicating the comparison outcome. Examples include:\n",
    "\n",
    "    Equal (==)\n",
    "    Not equal (!=)\n",
    "    Greater than (>)\n",
    "    Less than (<)\n",
    "    Greater than or equal (>=)\n",
    "    Less than or equal (<=)\n",
    "\n",
    "Comparison operations are useful for filtering data or making decisions within your model.\n",
    "\n",
    "3. Broadcasting:\n",
    "\n",
    "A powerful feature in PyTorch, broadcasting allows operations between tensors of different shapes as long as they are compatible. For instance, you can add a scalar value to a tensor, or add a one-dimensional tensor to a two-dimensional tensor (as long as the dimensions match). PyTorch automatically expands the smaller tensor to match the larger one for element-wise operations.\n",
    "\n",
    "4. In-place Operations:\n",
    "\n",
    "Certain operations modify the original tensor they are applied to, denoted by a trailing underscore (_). Examples include:\n",
    "\n",
    "    x.add_(y) (equivalent to x = x + y)\n",
    "    x.sub_(y) (equivalent to x = x - y)\n",
    "    x.mul_(y) (equivalent to x = x * y)\n",
    "\n",
    "These operations can be memory-efficient when modifying existing tensors is desired.\n",
    "\n",
    "5. Linear Algebra Operations:\n",
    "\n",
    "PyTorch provides functions for common linear algebra operations on tensors, including:\n",
    "\n",
    "    torch.matmul(a, b): Matrix multiplication between tensors a and b.\n",
    "    torch.sum(input, dim=None): Sums the elements of a tensor along a specified dimension.\n",
    "    torch.mean(input, dim=None): Computes the mean of the elements of a tensor along a specified dimension.\n",
    "\n",
    "These operations are essential for various deep learning tasks like calculating activation outputs or loss functions.\n",
    "\n",
    "6. Tensor Reshaping and Indexing:\n",
    "\n",
    "PyTorch offers functionalities to manipulate the shape and access specific elements of tensors:\n",
    "\n",
    "    x.view(new_shape): Reshapes the tensor x into a new shape while keeping the total number of elements the same.\n",
    "    x[index]: Accesses specific elements or sub-tensors using indexing syntax (similar to NumPy).\n",
    "\n",
    "Reshaping and indexing are crucial for preparing data for specific layers in your neural network architecture.\n",
    "\n",
    "7. Element-wise Operations:\n",
    "\n",
    "These operations apply a function to each element of a tensor independently. PyTorch provides a rich set of element-wise functions like:\n",
    "\n",
    "    torch.relu(x): Applies the rectified linear unit (ReLU) activation function.\n",
    "    torch.sigmoid(x): Applies the sigmoid activation function.\n",
    "    torch.tanh(x): Applies the hyperbolic tangent (tanh) activation function.\n",
    "\n",
    "Element-wise operations are fundamental for introducing non-linearity and transforming data in deep learning models.\n",
    "\n",
    "8. Random Operations:\n",
    "\n",
    "PyTorch offers functions for generating random tensors or modifying existing ones with randomness:\n",
    "\n",
    "    torch.rand(shape): Generates a random tensor filled with uniformly distributed values between 0 and 1.\n",
    "    torch.randn(shape): Generates a random tensor filled with values from a standard normal distribution.\n",
    "\n",
    "These operations are useful for data augmentation techniques or initializing weights in your network.\n",
    "\n",
    "By understanding and effectively using these PyTorch tensor operations, you can build and manipulate your deep learning models with greater flexibility and control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9nYSKhmoAQy"
   },
   "source": [
    "Some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1715159495582,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "DMm7DspXrWpj",
    "outputId": "f68d071f-a9ec-406f-8f37-72e5fead4739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tens: tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "Shape of tensor: torch.Size([2, 4])\n",
      "Datatype of tensor: torch.int64\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "lst_of_lsts = [[1, 2, 3, 4],[5, 6, 7, 8]]\n",
    "tens = torch.tensor(lst_of_lsts)\n",
    "print(f\"tens: {tens}\")\n",
    "print(f\"Shape of tensor: {tens.shape}\")\n",
    "print(f\"Datatype of tensor: {tens.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tens.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1715159495997,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "cLgpeDPeoDpG",
    "outputId": "cb5b2690-124e-446c-9962-6099aa162f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[[0.9502, 0.2374, 0.6583],\n",
      "         [0.4251, 0.5340, 0.6257]],\n",
      "\n",
      "        [[0.4119, 0.4152, 0.2960],\n",
      "         [0.0031, 0.7710, 0.5209]]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,2,3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1715159495997,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "5ADu57CsoykL",
    "outputId": "c29d4da1-ee2e-4874-ae32-f9eb80ab2430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t4.shape: torch.Size([300, 3])\n",
      "t5.shape: torch.Size([100, 9])\n"
     ]
    }
   ],
   "source": [
    "shpe = (100,3)\n",
    "t1 = torch.rand(shpe)\n",
    "t2 = torch.rand(shpe)\n",
    "t3 = torch.rand(shpe)\n",
    "t4=torch.cat([t1,t2,t3],dim=0)\n",
    "print(f\"t4.shape: {t4.shape}\")\n",
    "t5=torch.cat([t1,t2,t3],dim=1)\n",
    "print(f\"t5.shape: {t5.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1715159495997,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "NwV4Fnweo1OT",
    "outputId": "6b483281-9764-4a83-addb-246f1df7e08f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.3619,  0.1350, -1.0661,  ...,  0.9479,  0.4630, -0.6716],\n",
       "        [ 0.1350,  7.1206, -0.8727,  ...,  0.6149,  1.1330,  0.0441],\n",
       "        [-1.0661, -0.8727,  8.7019,  ...,  0.2642, -0.0942,  0.9023],\n",
       "        ...,\n",
       "        [ 0.9479,  0.6149,  0.2642,  ...,  8.9479, -0.2642, -0.0835],\n",
       "        [ 0.4630,  1.1330, -0.0942,  ..., -0.2642,  9.3356,  0.4979],\n",
       "        [-0.6716,  0.0441,  0.9023,  ..., -0.0835,  0.4979,  9.7197]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(100,100)\n",
    "X_cent = X - X.mean(dim=1, keepdim=True)\n",
    "covX = X_cent.T@X_cent\n",
    "covX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qKr9lixtGgO"
   },
   "source": [
    "## Data & Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TJNXJdfuHxH"
   },
   "source": [
    "### PyTorch Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZEnT7gHzWUp"
   },
   "source": [
    "Deep networks are versatile tools that can be adapted to various data types by\n",
    "leveraging appropriate pre-processing techniques and network architectures.\n",
    "PyTorch, like other deep learning libraries, can handle a wide array of data:\n",
    "  \n",
    "\n",
    "\n",
    "* images\n",
    "* audio\n",
    "* text data\n",
    "* tabluar (numerical, categorical, mixed)\n",
    "* multimodal Data\n",
    "* other\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbZ5lmtzuKlb"
   },
   "source": [
    "PyTorch also offers built-in vision specific datasets as part of the `torchvision.datasets` [module](https://pytorch.org/vision/stable/datasets.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9kGSr-D-y42B"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1715159499601,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "ML5of6dVzEAY",
    "outputId": "3b84c8d7-bfdf-4041-a0c6-c697a85b381f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CIFAR10',\n",
       " 'CIFAR100',\n",
       " 'CLEVRClassification',\n",
       " 'CREStereo',\n",
       " 'Caltech101',\n",
       " 'Caltech256',\n",
       " 'CarlaStereo',\n",
       " 'CelebA',\n",
       " 'Cityscapes',\n",
       " 'CocoCaptions',\n",
       " 'CocoDetection',\n",
       " 'Country211',\n",
       " 'DTD',\n",
       " 'DatasetFolder',\n",
       " 'EMNIST',\n",
       " 'ETH3DStereo',\n",
       " 'EuroSAT',\n",
       " 'FER2013',\n",
       " 'FGVCAircraft',\n",
       " 'FakeData',\n",
       " 'FallingThingsStereo',\n",
       " 'FashionMNIST',\n",
       " 'Flickr30k',\n",
       " 'Flickr8k',\n",
       " 'Flowers102',\n",
       " 'FlyingChairs',\n",
       " 'FlyingThings3D',\n",
       " 'Food101',\n",
       " 'GTSRB',\n",
       " 'HD1K',\n",
       " 'HMDB51',\n",
       " 'INaturalist',\n",
       " 'ImageFolder',\n",
       " 'ImageNet',\n",
       " 'Imagenette',\n",
       " 'InStereo2k',\n",
       " 'KMNIST',\n",
       " 'Kinetics',\n",
       " 'Kitti',\n",
       " 'Kitti2012Stereo',\n",
       " 'Kitti2015Stereo',\n",
       " 'KittiFlow',\n",
       " 'LFWPairs',\n",
       " 'LFWPeople',\n",
       " 'LSUN',\n",
       " 'LSUNClass',\n",
       " 'MNIST',\n",
       " 'Middlebury2014Stereo',\n",
       " 'MovingMNIST',\n",
       " 'Omniglot',\n",
       " 'OxfordIIITPet',\n",
       " 'PCAM',\n",
       " 'PhotoTour',\n",
       " 'Places365',\n",
       " 'QMNIST',\n",
       " 'RenderedSST2',\n",
       " 'SBDataset',\n",
       " 'SBU',\n",
       " 'SEMEION',\n",
       " 'STL10',\n",
       " 'SUN397',\n",
       " 'SVHN',\n",
       " 'SceneFlowStereo',\n",
       " 'Sintel',\n",
       " 'SintelStereo',\n",
       " 'StanfordCars',\n",
       " 'UCF101',\n",
       " 'USPS',\n",
       " 'VOCDetection',\n",
       " 'VOCSegmentation',\n",
       " 'VisionDataset',\n",
       " 'WIDERFace',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__getattr__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_optical_flow',\n",
       " '_stereo_matching',\n",
       " 'caltech',\n",
       " 'celeba',\n",
       " 'cifar',\n",
       " 'cityscapes',\n",
       " 'clevr',\n",
       " 'coco',\n",
       " 'country211',\n",
       " 'dtd',\n",
       " 'eurosat',\n",
       " 'fakedata',\n",
       " 'fer2013',\n",
       " 'fgvc_aircraft',\n",
       " 'flickr',\n",
       " 'flowers102',\n",
       " 'folder',\n",
       " 'food101',\n",
       " 'gtsrb',\n",
       " 'hmdb51',\n",
       " 'imagenet',\n",
       " 'imagenette',\n",
       " 'inaturalist',\n",
       " 'kinetics',\n",
       " 'kitti',\n",
       " 'lfw',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'moving_mnist',\n",
       " 'omniglot',\n",
       " 'oxford_iiit_pet',\n",
       " 'pcam',\n",
       " 'phototour',\n",
       " 'places365',\n",
       " 'rendered_sst2',\n",
       " 'sbd',\n",
       " 'sbu',\n",
       " 'semeion',\n",
       " 'stanford_cars',\n",
       " 'stl10',\n",
       " 'sun397',\n",
       " 'svhn',\n",
       " 'ucf101',\n",
       " 'usps',\n",
       " 'utils',\n",
       " 'video_utils',\n",
       " 'vision',\n",
       " 'voc',\n",
       " 'widerface']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CCQWfKR0etj"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21548,
     "status": "ok",
     "timestamp": 1715159521136,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "WQjlYm3I3bIj",
    "outputId": "57920d1c-ec39-42b5-cae6-399543c78cbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "train_source = training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "train_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1715159521136,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "uq7YInbH3z4K",
    "outputId": "e48518f9-08bc-42e5-d9d6-5e268c992440"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.FashionMNIST"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1715159521136,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "CI0VZXWe5oL_",
    "outputId": "de1c09d9-bbac-4434-9bdf-cb18a1a1129b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the data: torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"the shape of the data: {train_source.data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvCx0iKV5uPl"
   },
   "source": [
    "We can visualize the data using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UpM3phXL41eL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "executionInfo": {
     "elapsed": 1200,
     "status": "ok",
     "timestamp": 1715159522331,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "UGVwuqTP4w6B",
    "outputId": "d6f07905-b93d-4670-d361-393af111742a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 5, 2\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_source), size=(1,)).item()\n",
    "    img, label = train_source[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUArfaqcuF9p"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptoV6URJtMyg"
   },
   "source": [
    "As mentioned above, the `torch.utils.data.DataLoader` is responsible for handling data during the deep learning model training.\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "* dataset (Dataset): The PyTorch dataset you want to load data from (your custom class representing the data and how to access samples and labels).\n",
    "* batch_size (int, optional): The number of samples in a batch (default: 1).\n",
    "* shuffle (bool, optional): Whether to shuffle the data at the beginning of each epoch (default: False).\n",
    "* sampler (Sampler, optional): A custom sampler object for controlling data loading order (default: None).\n",
    "* collate_fn (callable, optional): A function to customize how samples within a batch are combined (default: None). Useful for padding sequences.\n",
    "* pin_memory=True (bool, optional): If using a GPU, pin fetched data tensors in pinned memory for faster transfer (default: False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1715159522331,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "4XyudTrorICU",
    "outputId": "ddd496c5-dcd2-4261-d075-36d868826431"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = DataLoader(train_source, batch_size=64, shuffle=True)\n",
    "type(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdKFXjkH6NPg"
   },
   "source": [
    "\n",
    "\n",
    "> The Dataset retrieves our datasets features and labels one sample at a time. While training a model, we typically want to pass samples in minibatches, reshuffle the data at every epoch to reduce model overfitting, and use Pythons multiprocessing to speed up data retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1715159522331,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "vBYbBhat6DBf",
    "outputId": "b1b3554a-845b-42c8-a40f-7c68f9c80c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n",
      "batch shape: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  btch = next(iter(train))\n",
    "  print(f\"batch shape: {btch[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOok4gOvaBuW"
   },
   "source": [
    "## Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3Fby9XPaJjR"
   },
   "source": [
    "Produce the following tensors:  \n",
    "\n",
    "1. Create a tensor from the list `[1, 2, 3]`\n",
    "2. Create a tensor from a NumPy array of shape (50,10)\n",
    "3. Specify a data type and device option for a tensor with `[7, 8, 9]`, `[.5,0,.7]`\n",
    "4. Create a zero tensor of size (3, 4)\n",
    "5. Create a ones tensor of size (2, 2, 2) with dtype float\n",
    "6. Create a tensor of size (5, 5) with random values from a normal distribution\n",
    "7. Create a new tensor by cloning an existing tensor\n",
    "8. Create a new tensor by reshaping an existing tensor\n",
    "9. Create a new tensor by concatenating two existing tensors\n",
    "10.  Perform operations with different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "ASqrBB6KeImk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. tensor from list: tensor([1, 2, 3])\n",
      "\n",
      "2. tensor from Numpy array (only shape): torch.Size([50, 10])\n",
      "\n",
      "3. Datatype of tensor: torch.float32\n",
      "\n",
      "3. Device tensor is stored on: cpu\n",
      "\n",
      "4. Zeros Tensor: tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]]) \n",
      "\n",
      "5. Ones Tensor: tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]]) \n",
      "\n",
      "6. Random Tensor from normal distributaion: tensor([[-0.9489, -0.4112, -1.4078, -0.6796, -1.5669],\n",
      "        [ 0.1291, -0.2364,  1.6805,  1.0250, -1.0534],\n",
      "        [-0.2484,  0.3496, -1.7422,  1.6472,  0.3874],\n",
      "        [-1.3664,  0.2172,  0.5761, -1.5206, -0.6895],\n",
      "        [-0.8343,  0.5739,  1.7445, -0.6033, -1.3787]]) \n",
      "\n",
      "7. New tensor created by cloning the random normal tensor: tensor([[-0.9489, -0.4112, -1.4078, -0.6796, -1.5669],\n",
      "        [ 0.1291, -0.2364,  1.6805,  1.0250, -1.0534],\n",
      "        [-0.2484,  0.3496, -1.7422,  1.6472,  0.3874],\n",
      "        [-1.3664,  0.2172,  0.5761, -1.5206, -0.6895],\n",
      "        [-0.8343,  0.5739,  1.7445, -0.6033, -1.3787]]) \n",
      "\n",
      "8. New tensor created by reshaping the ones tensor: tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "9. New tensor created by concatenating zeros_tensor and reshaped_tensor: tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.]])\n",
      "10. New tensor created by adding tensor_int and reshaped_tensor: tensor([[2., 3.],\n",
      "        [3., 5.],\n",
      "        [2., 4.],\n",
      "        [6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "my_lst = [1,2,3]\n",
    "tens_from_list = torch.tensor(my_lst)\n",
    "print(f\"1. tensor from list: {tens_from_list}\\n\")\n",
    "\n",
    "my_array = np.zeros((50, 10))\n",
    "tens_from_arr = torch.tensor(my_array)\n",
    "print(f\"2. tensor from Numpy array (only shape): {tens_from_arr.shape}\\n\")\n",
    "\n",
    "lst_for_tens = [[7, 8, 9], [.5,0,.7]]\n",
    "tens_from_lists = torch.tensor(lst_for_tens)\n",
    "print(f\"3. Datatype of tensor: {tens_from_lists.dtype}\\n\")\n",
    "print(f\"3. Device tensor is stored on: {tens_from_lists.device}\\n\")\n",
    "\n",
    "zeros_tensor = torch.zeros((2,4))\n",
    "print(f\"4. Zeros Tensor: {zeros_tensor} \\n\")\n",
    "\n",
    "ones_tensor = torch.ones((2,2,2), dtype=torch.float32)\n",
    "print(f\"5. Ones Tensor: {ones_tensor} \\n\")\n",
    "\n",
    "rand_tensor_normal = torch.randn((5, 5))\n",
    "print(f\"6. Random Tensor from normal distributaion: {rand_tensor_normal} \\n\")\n",
    "\n",
    "new_tensor = rand_tensor_normal.clone()\n",
    "print(f\"7. New tensor created by cloning the random normal tensor: {new_tensor} \\n\")\n",
    "\n",
    "reshaped_tensor = ones_tensor.view(2, 4)\n",
    "print(f\"8. New tensor created by reshaping the ones tensor: {reshaped_tensor} \\n\")\n",
    "\n",
    "concatenated_tensor = torch.cat([zeros_tensor,reshaped_tensor],dim=1)\n",
    "print(f\"9. New tensor created by concatenating zeros_tensor and reshaped_tensor: {concatenated_tensor}\")\n",
    "\n",
    "tensor_int = torch.tensor([[1, 2],[2,4],[1,3],[5,6]], dtype=torch.int32)\n",
    "\n",
    "diff_data_type = reshaped_tensor.view(4,2) + tensor_int\n",
    "print(f\"10. New tensor created by adding tensor_int and reshaped_tensor: {diff_data_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvL6c_cEboV_"
   },
   "source": [
    "1. Inspect the  attributes of some of the tensors you created with `shape,dtype,numel`.\n",
    "2. Indexing to access specific elements: `[[1, 2, 3], [4, 5, 6]]`, get the '2' element.\n",
    "\n",
    "3. Modify modify the '5' element to '10'\n",
    "4. Apply element-wise addition to two tensors.\n",
    "5. Apply element-wise multiplication to two tensors\n",
    "6. Apply a square root to a tensor.\n",
    "7. Create a tensor with random values from a uniform distribution between 0 and 1.\n",
    "8. Create a tensor with random values from a normal distribution with mean 0 and standard deviation 1\n",
    "9. Create a tensor with random values from a discrete uniform distribution between 1 and 10.\n",
    "10. Reshape a tensor from size `(2, 3)` to `(3, 2)`\n",
    "11. Transpose a tensor\n",
    "12. Concatenate tensors along a specific dimension\n",
    "13. Stack tensors along a new dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "d_eeWx27eLGJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Inspectation:\n",
      " shape of tens: torch.Size([3])\n",
      " tensor_int data type: torch.int32\n",
      " zeros_tensor numal:<built-in method numel of Tensor object at 0x000001F3C2C00A70>\n",
      "2. Indexing to access the two element: 2\n",
      "3. Modify the '5' element to '10': tensor([[ 1,  2,  3],\n",
      "        [ 4, 10,  6]])\n",
      "4. Apply element-wise addition to two tensors: tensor([[ 2,  4,  6],\n",
      "        [ 8, 20, 12]])\n",
      "5. Apply element-wise multiplication to two tensors: tensor([[  1,   4,   9],\n",
      "        [ 16, 100,  36]])\n",
      "6. Apply a square root to a tensor: tensor([[1.0000, 1.4142, 1.7321],\n",
      "        [2.0000, 3.1623, 2.4495]])\n",
      "7. tensor with random values from a uniform distribution between 0 and 1: tensor([[0.8077, 0.7635],\n",
      "        [0.2469, 0.7022]])\n",
      "8. tensor with random values from a normal distribution with mean 0 and standard deviation 1: tensor([[ 0.9118, -0.2230],\n",
      "        [-0.8981, -1.2668]])\n",
      "9. tensor with random values from a discrete uniform distribution between 1 and 10: tensor([[ 8,  4],\n",
      "        [10,  3]])\n",
      "10. Reshape a tensor from size (2, 3) to (3, 2):\n",
      " original:\n",
      " tensor([[ 0.4091, -2.6124, -0.7210],\n",
      "        [ 0.4686, -1.0523, -0.7528]]),\n",
      " reshaped:\n",
      " tensor([[ 0.4091, -2.6124],\n",
      "        [-0.7210,  0.4686],\n",
      "        [-1.0523, -0.7528]])\n",
      "11. Transpose tensor :\n",
      " original:\n",
      " tensor([[ 0.4091, -2.6124, -0.7210],\n",
      "        [ 0.4686, -1.0523, -0.7528]]),\n",
      " transposed:\n",
      " tensor([[ 0.4091,  0.4686],\n",
      "        [-2.6124, -1.0523],\n",
      "        [-0.7210, -0.7528]])\n",
      "12. Concatenated tensors along the 0 dimension:\n",
      " tensor([[ 0.8077,  0.7635],\n",
      "        [ 0.2469,  0.7022],\n",
      "        [ 0.9118, -0.2230],\n",
      "        [-0.8981, -1.2668]])\n",
      "13. Stacked tensors alonga new dimention:\n",
      " tensor([[[ 0.8077,  0.7635],\n",
      "         [ 0.2469,  0.7022]],\n",
      "\n",
      "        [[ 0.9118, -0.2230],\n",
      "         [-0.8981, -1.2668]]])\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(f\"1. Inspectation:\\n shape of tens: {tens_from_list.shape}\\n tensor_int data type: {tensor_int.dtype}\\n zeros_tensor numal:{zeros_tensor.numel}\")\n",
    "\n",
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"2. Indexing to access the two element: {my_tensor[0,1]}\")\n",
    "\n",
    "my_tensor[1,1]=10\n",
    "print(f\"3. Modify the '5' element to '10': {my_tensor}\")\n",
    "\n",
    "add_tensors = my_tensor + my_tensor\n",
    "print(f\"4. Apply element-wise addition to two tensors: {add_tensors}\")\n",
    "\n",
    "multiplication_tensors = my_tensor * my_tensor\n",
    "print(f\"5. Apply element-wise multiplication to two tensors: {multiplication_tensors}\")\n",
    "\n",
    "sqrt_tensor = torch.sqrt(my_tensor)\n",
    "print(f\"6. Apply a square root to a tensor: {sqrt_tensor}\")\n",
    "\n",
    "rand_unif = torch.rand((2,2))\n",
    "print(f\"7. tensor with random values from a uniform distribution between 0 and 1: {rand_unif}\")\n",
    "\n",
    "rand_normal = torch.randn((2,2))\n",
    "print(f\"8. tensor with random values from a normal distribution with mean 0 and standard deviation 1: {rand_normal}\")\n",
    "\n",
    "rand_unif_int = torch.randint(1, 11, (2, 2))\n",
    "print(f\"9. tensor with random values from a discrete uniform distribution between 1 and 10: {rand_unif_int}\")\n",
    "\n",
    "tensor_2_3 = torch.randn((2,3))\n",
    "reshapes_tensor_2_3 = tensor_2_3.view(3,2)\n",
    "print(f\"10. Reshape a tensor from size (2, 3) to (3, 2):\\n original:\\n {tensor_2_3},\\n reshaped:\\n {reshapes_tensor_2_3}\")\n",
    "\n",
    "transposed_tensor_2_3 = tensor_2_3.t()\n",
    "print(f\"11. Transpose tensor :\\n original:\\n {tensor_2_3},\\n transposed:\\n {transposed_tensor_2_3}\")\n",
    "\n",
    "Concatenate_tensors = torch.cat((rand_unif, rand_normal), dim=0)\n",
    "print(f\"12. Concatenated tensors along the 0 dimension:\\n {Concatenate_tensors}\")\n",
    "\n",
    "stack_tensors = torch.stack((rand_unif, rand_normal), dim=0)\n",
    "print(f\"13. Stacked tensors alonga new dimention:\\n {stack_tensors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5Ag4aLvd186"
   },
   "source": [
    "1. Sum of all elements in a tensor.\n",
    "2. Mean along a specific dimension\n",
    "3. Find the minimum and maximum values in a tensor\n",
    "4. Find the indices of minimum and maximum values in a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "YS_zcuzPeL2y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Sum of all elements in my tensor:\n",
      " 26\n",
      "2. Mean along the 0 dimension:\n",
      " tensor([2.5000, 6.0000, 4.5000])\n",
      "3.\n",
      " Minimum value in my_tensor: 1\n",
      "\n",
      " Maximum value in my_tensor: 10\n",
      "4.\n",
      " Index of minimum value in my_tensor: 0\n",
      "\n",
      " Index of maximum value in my_tensor: 4\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "my_tensor_sum = torch.sum(my_tensor)\n",
    "print(f\"1. Sum of all elements in my tensor:\\n {my_tensor_sum}\")\n",
    "\n",
    "my_tensor_float = my_tensor.float() \n",
    "my_tensor_mean = torch.mean(my_tensor_float, dim=0)\n",
    "print(f\"2. Mean along the 0 dimension:\\n {my_tensor_mean}\")\n",
    "\n",
    "my_tensor_min = torch.min(my_tensor)\n",
    "print(f\"3.\\n Minimum value in my_tensor: {my_tensor_min}\")\n",
    "my_tensor_max = torch.max(my_tensor)\n",
    "print(f\"\\n Maximum value in my_tensor: {my_tensor_max}\")\n",
    "\n",
    "my_tensor_min_index = torch.argmin(my_tensor)\n",
    "print(f\"4.\\n Index of minimum value in my_tensor: {my_tensor_min_index}\")\n",
    "my_tensor_max_index = torch.argmax(my_tensor)\n",
    "print(f\"\\n Index of maximum value in my_tensor: {my_tensor_max_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUhF40qoc9lh"
   },
   "source": [
    "Broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1715159522332,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "WZ3jXo9F7YyB",
    "outputId": "09168315-7d03-4d52-ebde-4ff6d35f9051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting with scalars:\n",
      "tensor([6, 7, 8])\n",
      "Broadcasting with different shapes:\n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Broadcasting with scalars\n",
    "tensor_a = torch.tensor([1, 2, 3])\n",
    "scalar_b = 5\n",
    "result_broadcast_scalar = tensor_a + scalar_b\n",
    "print(\"Broadcasting with scalars:\")\n",
    "print(result_broadcast_scalar)  # Output: tensor([6, 7, 8])\n",
    "\n",
    "# Broadcasting with different shapes\n",
    "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_d = torch.tensor([10, 20, 30])\n",
    "result_broadcast_shape = tensor_c + tensor_d\n",
    "print(\"Broadcasting with different shapes:\")\n",
    "print(result_broadcast_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 458,
     "status": "ok",
     "timestamp": 1715159522778,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "CTTx-zG3dFzd",
    "outputId": "c0b03603-38b7-46c1-993b-9d66020f4863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcasting with multidimensional tensors:\n",
      "tensor([[11, 22, 33],\n",
      "        [14, 25, 36]])\n",
      "RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# Broadcasting with multidimensional tensors\n",
    "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_b = torch.tensor([10, 20, 30])\n",
    "result_broadcast = tensor_a + tensor_b\n",
    "print(\"Broadcasting with multidimensional tensors:\")\n",
    "print(result_broadcast)\n",
    "\n",
    "# Common broadcasting pitfalls\n",
    "tensor_c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_d = torch.tensor([10, 20])\n",
    "# The following line will raise a RuntimeError\n",
    "try:\n",
    "    result_pitfall = tensor_c + tensor_d\n",
    "except RuntimeError as e:\n",
    "    print(\"RuntimeError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8XnR20UdL9x"
   },
   "source": [
    "Device specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1715159522778,
     "user": {
      "displayName": "Lee Carlin",
      "userId": "17523197383808952175"
     },
     "user_tz": -180
    },
    "id": "UmOPlcDndLei",
    "outputId": "41ccd2a3-1a28-4fd6-b70a-d7861c6cb337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available? False\n",
      "Tensor on GPU: tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Code examples for device configuration and availability\n",
    "\n",
    "# Check if GPU is available\n",
    "is_gpu_available = torch.cuda.is_available()\n",
    "print(\"Is GPU available?\", is_gpu_available)\n",
    "\n",
    "# Specify device for tensor operations\n",
    "device = torch.device('cuda' if is_gpu_available else 'cpu')\n",
    "tensor_gpu = torch.tensor([1, 2, 3], device=device)\n",
    "print(\"Tensor on GPU:\", tensor_gpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTb9_VWsgCkv"
   },
   "source": [
    "Dataloader practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-XSmZ7Tffq2"
   },
   "source": [
    "Using the Fashion MNIST dataloader object, iterate over the batches, and calculate an interesting statistic. Make sure the statistic represent the entire data, not just a single batch. Hint- find a way to \"update\" the statistic along the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "ED4PArw7e43C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean pixel value of the Fashion MNIST dataset: 0.28604090213775635\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from torchvision.transforms import ToTensor\n",
    "data_source = data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "batch_size = 32  \n",
    "data_loader = DataLoader(train_source, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "cum_sum =0 \n",
    "tot_count = 0\n",
    "for images, labels in data_loader:\n",
    "    cum_sum += images.sum()\n",
    "    tot_count += images.numel()\n",
    "\n",
    "mean_pixel_value = cum_sum / tot_count\n",
    "\n",
    "print(\"Mean pixel value of the Fashion MNIST dataset:\", mean_pixel_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrhHnCUOXeTY"
   },
   "source": [
    "## How to export an `HTML` file of your `ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yvoSJjmXqah"
   },
   "source": [
    "1. Save your notebook and make sure all the cells have the expected output.\n",
    "2. Download your notebook to your local machine.   `File-->Download-->Download ipynb`\n",
    "3.  Reupload it so Colab can see it :  Click on the Files icon on the far left bar--> you should see your current kernel folder --> click `upload to session storage` --> upload your file.\n",
    "4. Execute the following: `!jupyter nbconvert --to html /content/NOTEBOOKFILE.ipynb`\n",
    "5. You should see the html file produced in your kernel's current folder. You can download it locally.\n",
    "[link text](https://)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKpF6F5E741a"
   },
   "source": [
    " END OF PyTorch BASICS"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
